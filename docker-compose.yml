version: '3.8'

services:
  bot:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: telegram-ollama-bot
    restart: unless-stopped
    environment:
      - BOT_TOKEN=${BOT_TOKEN}
      - DEFAULT_LANGUAGE=${DEFAULT_LANGUAGE:-en}
      - DEV=${DEV:-true}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-llama3.2}
      # Use the Ollama service name as host when connecting to Ollama
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    depends_on:
      - ollama
    # Create a network with the Ollama service
    networks:
      - ollama-network

  # Ollama service - if you already have Ollama running elsewhere, you can remove this
  # service and update the OLLAMA_HOST environment variable above
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-service
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
    networks:
      - ollama-network
    environment:
      - OLLAMA_MODELS=/root/.ollama/models

networks:
  ollama-network:
    driver: bridge

volumes:
  ollama-data:
    driver: local 